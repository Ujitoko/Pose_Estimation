{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この姿勢推定プログラムを書いているのは常磐線の中である．  \n",
    "戸塚→勝田移動の残業代の出ないのに拘束される時間を使っている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAD8CAYAAADzEfagAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e579208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img_src_path = \"depth32/0_gen_dep_20160712194620972.png\"\n",
    "depth_img = Image.open(img_src_path) \n",
    "\n",
    "# 画像表示\n",
    "img_np = np.array(depth_img)\n",
    "plt.imshow(img_np, cmap=\"gray\")\n",
    "\n",
    "# 画像サイズ\n",
    "print(img_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert imgs into .npy files\n",
    "def ConvertIntoNpy(src, dst):\n",
    "    files = os.listdir(src)\n",
    "    first_img = Image.open(src + \"/\" + files[0])\n",
    "    first_img_np = np.array(first_img)\n",
    "    file_list = np.copy(first_img_np)\n",
    "    file_list = file_list[np.newaxis, :]\n",
    "    \n",
    "    print(\"num of files: \" + str(len(files)))\n",
    "    print(\"img size       : \" + str(first_img_np.shape))\n",
    "    \n",
    "    for i in range(len(files)):\n",
    "        if i % 100 == 1:\n",
    "            print(i)\n",
    "            \n",
    "        img = Image.open(src + \"/\" + files[i])\n",
    "        img_np = np.array(img)\n",
    "\n",
    "        # 念のため画像サイズチェック\n",
    "        if img_np.shape != first_img_np.shape:\n",
    "            continue\n",
    "            \n",
    "        img_np = img_np[np.newaxis, :]\n",
    "        file_list = np.vstack((file_list, img_np))\n",
    "        img.close()\n",
    "        \n",
    "    np.save(dst, fileList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Pose_Estimation_Net:\n",
    "    def __init__(self):\n",
    "        self.epochs = 100000\n",
    "        self.epoch_saveMetrics = 500\n",
    "        #self.epoch_saveSampleImg = 500\n",
    "        self.epoch_saveParameter = 1000\n",
    "        \n",
    "        self.X_tr = tf.placeholder(tf.float32, shape=[None, 240, 240])\n",
    "        self.Y_tr = tf.placeholder(tf.float32, shape=[None, 22, 3])\n",
    "\n",
    "        self.batch_size = 32\n",
    "        img_size = 240*240 # orginal size:640*480\n",
    "\n",
    "        self.L = self.loss()\n",
    "        self.losses = {\"loss\":[]}\n",
    "\n",
    "        # model architecture\n",
    "    def output(self):\n",
    "        with tf.variable_scope(\"Pose_Estimation_Net\"):\n",
    "            # x:[240*240*1]\n",
    "            conv1 = tf.layers.conv2d(x, 3, [4,4], [2,2], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02)) # 120x120x3\n",
    "            conv1 = tf.layers.batch_normalization(conv1)\n",
    "            conv1 = leaky_relu(conv1)\n",
    "\n",
    "            conv2 = tf.layers.conv2d(conv1, 6, [4,4], [2,2], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02)) # 60x60x6\n",
    "            conv2 = tf.layers.batch_normalization(conv2)\n",
    "            conv2 = leaky_relu(conv2)\n",
    "\n",
    "            conv3 = tf.layers.conv2d(conv2, 12, [4,4], [2,2], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02)) # 30x30x12\n",
    "            conv3 = tf.layers.batch_normalization(conv3)\n",
    "            conv3 = leaky_relu(conv3)\n",
    "            \n",
    "            conv4 = tf.layers.conv2d(conv3, 24, [4,4], [2,2], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02)) # 15x15x24\n",
    "            conv4 = tf.layers.batch_normalization(conv4)\n",
    "            conv4 = leaky_relu(conv4)\n",
    "\n",
    "            fc1 = tf.reshape(conv4, [-1, 15*15*24])\n",
    "            fc1 = tf.layers.dense(fc1, 1024, kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02)) # noise\n",
    "            fc1 = tf.layers.batch_normalization(fc1)\n",
    "            fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "            fc2 = tf.layers.dense(fc1, 2048, kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02)) # noise\n",
    "            fc2 = tf.layers.batch_normalization(fc2)\n",
    "            fc2 = tf.nn.relu(fc2)\n",
    "\n",
    "            fc3 = tf.layers.dense(fc2, 3*22, kernel_initializer=tf.truncated_normal_initializer(0.0, 0.02))\n",
    "\n",
    "            y = tf.reshape(fc3, [-1, 3*22])\n",
    "        return y\n",
    "    def loss(self):\n",
    "        output = self.output(self.X_tr)\n",
    "        L = self.Y_tr - output\n",
    "        L = tf.square(L)\n",
    "        L = tf.reduce_mean(L)\n",
    "        return L\n",
    "        \n",
    "    def train(self):\n",
    "        opt = tf.train.AdamOptimizer()\n",
    "        train_op = opt.minimize(self.L)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            for epoch in range(self.epochs):\n",
    "                # 訓練データを抜粋\n",
    "                # X_mb\n",
    "                \n",
    "                def extractImg(img_path, label_path):\n",
    "                    files_img = os.listdir(img_path)\n",
    "                    rand_index = np.random.randint(0, len(files_img), size=self.batch_size)\n",
    "                    # files_img[rand_index]\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    files_label[rand_index]\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                X_mb_src_path = \"./depth32\"\n",
    "                extractImg(X_mb_src_path)\n",
    "                # Y_mb\n",
    "                \n",
    "                # 正規化\n",
    "                _, loss_value = sess.run([train_op, self.L], feed_dict={self.X_tr: X_mb, self.Y_tr: Y_mb})\n",
    "\n",
    "                self.losses[\"loss\"].append(loss_value)\n",
    "                \n",
    "                # lossの可視化\n",
    "                if epoch % self.epoch_saveMetrics == 1:\n",
    "                    save_metrics(self.losses, epoch)\n",
    "\n",
    "                # parameterのsave\n",
    "                if epoch % self.epoch_saveParamter == 1:\n",
    "                    path = \"model_DC\"\n",
    "                    if not os.path.isdir(path):\n",
    "                        os.makedirs(path)\n",
    "\n",
    "                    saver.save(sess, path+\"/dcgan_model\" + str(epoch+epoch_pre) + \".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Pose_Estimation_Net()\n",
    "model.train()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
